{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from keras import models,layers\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#import preparation,evaluation,logistic,SVC,naive,randomForest,XGboost,ADAboost,NeuralNet,result\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# 評価関数\n",
    "# accuracy\n",
    "score = 'accuracy'\n",
    "\n",
    "\n",
    "# アルゴリズム\n",
    "# logisticRegression\n",
    "# SVC\n",
    "# naiveBayes\n",
    "# randomForest\n",
    "# xgboost\n",
    "# adaboost\n",
    "# neuralNet\n",
    "# KNeighborsClassifier\n",
    "\n",
    "arg = ['SVC','randomForest','KNeighborsClassifier']\n",
    "\n",
    "\n",
    "# IDと答えが同じテーブルにないとき\n",
    "train = list(range(1,X_train[:,0].size+1))\n",
    "train = pd.concat([pd.DataFrame(train),pd.DataFrame(Y_train[:]),pd.DataFrame(X_train[:])],axis = 1)\n",
    "train.columns =['ID','answer','gakulen','gakuwide','hanalen','hanawide']\n",
    "\n",
    "test = list(range(1,X_test[:,0].size+1))\n",
    "test = pd.concat([pd.DataFrame(test),pd.DataFrame(X_test[:,0:])],axis = 1)\n",
    "test.columns =['ID','gakulen','gakuwide','hanalen','hanawide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flower(num):\n",
    "    ''' 数字を受け取って、対応する名前を返します。'''\n",
    "    if num == 0:\n",
    "        return 'Setosa'\n",
    "    elif num == 1:\n",
    "        return 'Veriscolour'\n",
    "    else:\n",
    "        return 'Virginica'\n",
    "\n",
    "train['answer'] = train['answer'].apply(flower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fujii\\Anaconda3\\envs\\tesorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\fujii\\Anaconda3\\envs\\tesorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\fujii\\Anaconda3\\envs\\tesorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\fujii\\Anaconda3\\envs\\tesorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価指標：accuracy\n",
      "\n",
      "アルゴリズムランキング\n",
      "1位：ランダムフォレスト（0.9618736383442266）\n",
      "2位：K近傍法（0.9531590413943355）\n",
      "3位：サポートベクトルマシン（0.9240273887332711）\n"
     ]
    }
   ],
   "source": [
    "# データの準備\n",
    "train_x, train_y, test, test_ID, answer_num = preparation(train, test)\n",
    "\n",
    "# 正規化\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test = scaler.fit_transform(test)\n",
    "\n",
    "#　アルゴリズムの結果を格納するリスト\n",
    "ranking = {}\n",
    "\n",
    "ranking = evaluation(train_x, train_y, ranking, answer_num, arg)\n",
    "\n",
    "# アルゴリズムをランキング形式で並べ替え\n",
    "ranking = sorted(ranking.items(), key=lambda x:-x[1])\n",
    "\n",
    "# 結果\n",
    "result(train_x, train_y, test, test_ID, score, answer_num,  ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation(train, test):\n",
    "    \n",
    "    # trainとtestを結合する前に判別フラグを立てる\n",
    "    train['train_flg'] = 1\n",
    "    test['train_flg'] = 0\n",
    "    \n",
    "    # testに答えの列を追加\n",
    "    test[train.columns[1]] = 9\n",
    "    \n",
    "    # trainとtestを結合\n",
    "    #train_test_combine = pd.concat([train.drop(train.columns[1], axis=1),test],axis=0)\n",
    "    train_test_combine = pd.concat([train,test],axis=0)\n",
    "\n",
    "    # 削除する行名と列名の格納するリストを定義\n",
    "    delete_row = []\n",
    "    delete_column = []\n",
    "\n",
    "    # 欠損値の割合を計算\n",
    "    percent = 100 * train_test_combine.isnull().sum()/ len(train_test_combine)      \n",
    "\n",
    "    # 特徴量の数だけループ\n",
    "    for i in range(percent.size):\n",
    "        # object型かつカラムに30種類以上のデータが含まれる、または、欠損値が全体の15％以上ある場合列を削除\n",
    "        if (train_test_combine[percent.index[i]].dtype == ('object') \n",
    "        and train_test_combine[percent.index[i]].nunique() >= 30)or percent[i] >= 15:\n",
    "            delete_column.append(percent.index[i])\n",
    "        # 欠損値が存在し、全体の15％以下である場合行を削除\n",
    "        elif percent[i] > 0 and percent[i] < 15:\n",
    "            delete_row.append(percent.index[i])\n",
    "    \n",
    "    # 行の削除\n",
    "    for i in delete_row:\n",
    "        train_test_combine = train_test_combine[train_test_combine[i].notnull()]\n",
    "    # 列の削除\n",
    "    for j in delete_column:\n",
    "        train_test_combine = train_test_combine.drop([j], axis=1)\n",
    "        \n",
    "    # trainの答えを退避\n",
    "    train_y = train_test_combine[train_test_combine['train_flg'] == 1][train.columns[1]]\n",
    "    # testのIDを退避\n",
    "    test_ID = train_test_combine[train_test_combine['train_flg'] == 0][test.columns[0]]\n",
    "    # 答えの種類の数を退避\n",
    "    answer_num = train[train.columns[1]].nunique()\n",
    "    \n",
    "    # ID, 答えを削除\n",
    "    train_test_combine = train_test_combine.drop([train.columns[0],train.columns[1]],axis=1) \n",
    "    \n",
    "    # カテゴリカル変数の特徴量だけを抽出\n",
    "    categorical_columns = [c for c in train_test_combine.columns if (train_test_combine[c].dtype != ('float64') and train_test_combine[c].dtype != ('int64') and train_test_combine[c].dtype != ('bool'))]\n",
    "\n",
    "    # カテゴリカル変数の種類と個数を出力し、one-hot-encoding\n",
    "    for col in categorical_columns:\n",
    "        ohe = pd.get_dummies(train_test_combine[col],drop_first=True)\n",
    "        train_test_combine = train_test_combine.drop([col],axis=1)\n",
    "        train_test_combine = pd.concat([train_test_combine,ohe], axis=1)\n",
    "        \n",
    "    train = train_test_combine[train_test_combine['train_flg'] == 1].drop(['train_flg'], axis=1)\n",
    "    test = train_test_combine[train_test_combine['train_flg'] == 0].drop(['train_flg'], axis=1)\n",
    "    \n",
    "    return train, train_y, test, test_ID, answer_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(x, y, ranking, answer_num, arg):\n",
    "\n",
    "    for i in arg:\n",
    "        if i == 'logisticRegression':\n",
    "            ranking = logistic(x, y, ranking)\n",
    "        elif i == 'SVC':\n",
    "            ranking = SVCK(x, y, ranking)\n",
    "        elif i == 'naiveBayes':\n",
    "            ranking = naive(x, y, ranking)\n",
    "        elif i == 'randomForest':\n",
    "            ranking = randomForest(x, y, ranking)\n",
    "        elif i == 'xgboost':\n",
    "            ranking = XGboost(x, y, ranking)\n",
    "        elif i == 'adaboost':\n",
    "            ranking = ADAboost(x, y, ranking)\n",
    "        elif i == 'neuralNet':\n",
    "            ranking = NeuralNet(x, y, answer_num, ranking)\n",
    "        elif i == 'KNeighborsClassifier':\n",
    "            ranking = KNeighber(x, y, ranking)\n",
    "        else:\n",
    "            print(i + 'は対応していません')\n",
    "    \n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x, y, ranking):\n",
    "    #ロジスティック回帰\n",
    "\n",
    "    logistic_regression = LogisticRegressionCV(random_state=0, penalty='l2', Cs=10, multi_class='ovr',n_jobs=-1)\n",
    "    answer = cross_val_score(logistic_regression, x, y, scoring = score)\n",
    "    ranking['ロジスティック回帰'] = np.average(answer)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCK(x, y, ranking):\n",
    "    #サポートベクトルマシン\n",
    "\n",
    "    svc = SVC(C=1.0,kernel='linear',probability=True,random_state=0)\n",
    "    answer = cross_val_score(svc, x, y, scoring = score)\n",
    "    ranking['サポートベクトルマシン'] = np.average(answer)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive(x, y, ranking):\n",
    "    #ナイーブベイズ\n",
    "\n",
    "    naive = GaussianNB()\n",
    "    answer = cross_val_score(naive, x, y, scoring = score)\n",
    "    ranking['ナイーブベイズ'] = np.average(answer)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(x, y, ranking):\n",
    "    #ランダムフォレスト\n",
    "\n",
    "    RF = RandomForestClassifier(criterion='gini', n_estimators=25, random_state=0, n_jobs=-1)\n",
    "    answer = cross_val_score(RF, x, y, scoring = score)\n",
    "    ranking['ランダムフォレスト'] = np.average(answer)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGboost(x, y, ranking):\n",
    "    #Xgboost\n",
    "\n",
    "    xgboost = xgb.XGBClassifier()\n",
    "    answer = cross_val_score(xgboost, x, y, scoring = score)\n",
    "    ranking['XGブースト'] = np.average(answer)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAboost(x, y, ranking):\n",
    "    #ADAブースト\n",
    "\n",
    "    adaboost = AdaBoostClassifier(random_state=0)\n",
    "    answer = cross_val_score(adaboost, x, y, scoring =score)\n",
    "    ranking['ADAブースト'] = np.average(answer)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNeighber(x, y, ranking):\n",
    "    #K近傍法\n",
    "\n",
    "    Knn = KNeighborsClassifier()\n",
    "    answer = cross_val_score(Knn, x, y, scoring =score)\n",
    "    ranking['K近傍法'] = np.average(answer)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNet(x, y, answer_num, ranking):\n",
    "    # ニューラルネット\n",
    "\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    y_train_ohe = pd.get_dummies(y_train,drop_first=False)\n",
    "    y_test_ohe = pd.get_dummies(y_test,drop_first=False)\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(units=32, activation='relu', input_shape=(X_train[0].size,)))\n",
    "    model.add(layers.Dense(units=64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(units=answer_num, activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer = \"rmsprop\", metrics=[score])\n",
    "\n",
    "    # 早期終了を行うコールバック関数を設定\n",
    "    callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2)]\n",
    "\n",
    "    history = model.fit(X_train, y_train_ohe, epochs=20, verbose=0, callbacks = callbacks, batch_size=100) \n",
    "    loss, scores = model.evaluate(X_test, y_test_ohe)\n",
    "\n",
    "    ranking['ニューラルネット'] = scores\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(x, y, xt, test_ID, score, answer_num, ranking):\n",
    "\n",
    "    print(\"評価指標：\" + score)\n",
    "    print()\n",
    "    print(\"アルゴリズムランキング\")\n",
    "    j = 1\n",
    "\n",
    "    for k, i in ranking:\n",
    "        print(str(j) + \"位：\" + k,end=\"\")\n",
    "        print(\"（\" ,end=\"\")\n",
    "        print(i, end=\"\")\n",
    "        print(\"）\")\n",
    "        if j == 1:\n",
    "            if k == 'ニューラルネット':\n",
    "                \n",
    "                y_ohe = pd.get_dummies(y,drop_first=False)\n",
    "\n",
    "                model = models.Sequential()\n",
    "                model.add(layers.Dense(units=32, activation='relu', input_shape=(X_train[0].size,)))\n",
    "                model.add(layers.Dense(units=64, activation='relu'))\n",
    "                model.add(layers.Dropout(0.2))\n",
    "                model.add(layers.Dense(units=answer_num, activation='softmax'))\n",
    "                model.compile(loss=\"categorical_crossentropy\", optimizer = \"rmsprop\", metrics=[score])\n",
    "\n",
    "                callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "                             ModelCheckpoint(filepath = \"best_model.h5\",monitor = \"val_loss\", save_best_only = True)]\n",
    "                model.fit(x, y_ohe, epochs=20, verbose=0, callbacks = callbacks, batch_size=100, validation_split=0.3) \n",
    "                predicted = model.predict(xt)\n",
    "                submit= pd.concat([test_ID, pd.DataFrame(predicted)], axis = 1)\n",
    "                if os.path.isfile('best_model.pkl'):\n",
    "                    os.remove('best_model.pkl')\n",
    "                \n",
    "            else:\n",
    "                if k == 'ロジスティック回帰':\n",
    "                    model = LogisticRegressionCV(random_state=0, penalty='l2', Cs=10, n_jobs=-1).fit(x,y)\n",
    "                elif k == 'サポートベクトルマシン':\n",
    "                    model = SVC(C=1.0,kernel='linear',probability=True,random_state=0).fit(x,y)\n",
    "                elif k == 'ナイーブベイズ':\n",
    "                    model = CalibratedClassifierCV(GaussianNB(), cv=2, method='softmax')\n",
    "                elif k == 'ランダムフォレスト':\n",
    "                    model = RandomForestClassifier(criterion='gini', n_estimators=25, random_state=0, n_jobs=-1).fit(x,y)\n",
    "                elif k == 'XGブースト':\n",
    "                    model = xgb.XGBClassifier().fit(x,y)\n",
    "                elif k == 'ADAブースト':\n",
    "                    model = AdaBoostClassifier(random_state=0).fit(x,y)\n",
    "                elif k == 'K近傍法':\n",
    "                    model = KNeighborsClassifier().fit(x,y)\n",
    "\n",
    "                joblib.dump(model, \"best_model.pkl\")\n",
    "                predicted = model.predict_proba(xt)\n",
    "                submit = pd.concat([test_ID, pd.DataFrame(predicted)],axis = 1)\n",
    "                if os.path.isfile('best_model.h5'):\n",
    "                    os.remove('best_model.h5')\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    submit.to_csv(\"submit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
